function [p1_pe,p1_a_est,p1_b_est,p1_b_var] = HS2018_SysID_final_p1_15819790(p1_u,p1_y)
close all
clc
%% --------------------------------------------------------------------- %%
%% System Identification Midterm Problem 1
%% --------------------------------------------------------------------- %%

%% My details
disp(' ');
disp('    ###########################################################')
disp('    #          SYSTEM IDENTIFICATION FINAL PROBLEM 1          #')
disp('    #              Student Name: Charlotte Moraldo            #')
disp('    #                Legi Number: 15-819-790                  #')
disp('    ###########################################################')
disp(' ');


%% --------------------------------------------------------------------- %%
%% PART 1 
%% --------------------------------------------------------------------- %%
disp(' ');
disp('#####################################################################')
disp('#                             PART 1:                               #')
disp('#     Order of the persistency of excitation of the input signal    #')
disp('#####################################################################')
disp(' ');

N = length(p1_u);
n_samples = seqperiod(p1_u);
n_per_notround = size(p1_u,1)/seqperiod(p1_u);
n_per = floor(size(p1_u,1)/seqperiod(p1_u));
explanation0 = [...
'The input signal p1_u contains N=',num2str(N),' samples. When plotting it,\n'...
'we can see that it is periodic. I can therefore compute its periodicity\n' ...
'characteristics: \n' ...
'    > The number of samples per period is obtained with the matlab command\n'...
'      seqperiod(p1_u), where p1_u is given. I obtain: n_samples = ',num2str(n_samples),'\n'...
'    > The number of periods is obtained through the division of the size \n'...
'      of p1_u with the number of samples per period (n_samples): n_per = ',num2str(n_per),'\n'...
    ];
fprintf(explanation0);
disp(' ');

u_crop = p1_u(1:n_samples);
U_crop = fft(u_crop);
phi_u = abs(U_crop).^2/n_samples;
non_zero = nnz(phi_u);

p1_pe = non_zero;

explanation1 = [...
'As explained in slides 9.19-9.20, an input u(k) is persistently exciting \n'...
'of order n if the matrix containing its autocorrelation coefficients \n'...
'(Toeplitz matrix) is positive definite. An equivalent method to describe \n'...
'persistency of excitation is to look at the spectral density of the signal: \n'...
'what it means in the frequency domain, is that a signal is persistently \n'...
'exciting of order n if there are at least n non-zero frequencies (positive \n'...
'and negative frequencies).\n'... 
'In the case of a periodic signal, there is only a finite number of frequencies \n'...
'we can get because a periodic signal is persistently exciting up to a certain \n'...
'order. Using the autocorrelation and spectral density formula for periodic \n'...
'signals (slide 2.19), I computed the spectral density of the input p1_u over \n'...
'its first period: phi_u = 1/n_samples * |U(e^jw)|^2, where U is the fft of p1_u.\n'... 
'Using the Matlab command nnz(phi_u), I found that the number of non-zero \n'...
'frequencies is equal to ',num2str(p1_pe),'. Therefore, the order of \n'...
'persistency of the input signal is equal to: p1_pe = ',num2str(p1_pe),'.\n'...
'\n'];
fprintf(explanation1);
disp(' ');

disp(' ');



%% --------------------------------------------------------------------- %%
%% PART 2 
%% --------------------------------------------------------------------- %%
disp(' ');
disp('#####################################################################')
disp('#                             PART 2:                               #')
disp('#          Estimate of the parameters theta = (a1, a2, b1)          #')
disp('#####################################################################')
disp(' ');

% Transfer function definitions
c0 = 1; c1 = -2.25; c2 = 1.5625;

% Noise statistics
lambda = 0.03;

% Define parameters vector
nb_params = 3;
thetaLS = ones(nb_params,1); %(a1,a2,b1)
thetaBL = ones(nb_params,1); %(a1,a2,b1)




%% BLUE Estimate
% Error covariance matrix
for k = 1:N
    for l = 1:N
        if k == 1 && l == 1
            R(k,l) = lambda;
        elseif k == 2 && l == 2
            R(k,l) = (1+c1^2)*lambda;
        elseif k == l && k >= 3
            R(k,l) = (1+c1^2+c2^2)*lambda;
        elseif (k == 1 && l == 2) || (k == 2 && l == 1)
            R(k,l) = c1*lambda;
        elseif abs(k-l) == 1 && min(k,l) >= 2
            R(k,l) = (c1+c1*c2)*lambda;  
        elseif abs(k-l) == 2
            R(k,l) = c2*lambda;
        elseif abs(k-l) >= 3
            R(k,l) = 0;
        else
            disp('Error in covariance matrix')
        end
    end
end

% Create regressor
phi = zeros(N-2,nb_params);
for k=3:N
    phi(k-2,:) = [-p1_y(k-1), -p1_y(k-2), p1_u(k-1)];
end

% Crop sigma and regressor
R(1:2,:) = [];
R(:,1:2) = [];

%% Linear Estimator
R_inv = R\eye(N-2);
M = phi.'*R_inv*phi;
M_inv = M\eye(nb_params);
Z = R_inv * phi * M_inv;

thetaBL = Z.'*p1_y(3:end);

%% Return estimates of a and b
p1_a_est = thetaBL(1:2);
p1_b_est = thetaBL(3);


%% Documentation
explanation2 = [...
'The model that we are considering in this exercise has the form ARMAX \n'...
'(Autoregressive Moving Average Model): y(k) = B/A u(k) + C/A e(k).\n'...
'\n'...
'In the case where the noise model is known, it is possible to bring the \n'...
'ARMAX model to the ARX model form (slide 10.39). This was done in exercise \n'...
'set 8: since C was exactly known by assumption, it was possible to check \n'...
'that C was stably invertible and therefore bring the system to an ARX form \n'...
'by filtering the signal via the inverse of C. The unbiased estimate of \n'...
'theta was then simply found by performing least squares.  \n'...
'At first, we could think that this could be applicable to our identification \n'...
'problem: C is known, and it is given that e(k) are i.i.d random variables\n'...
'drawn from N(0,0.03). However, we can quickly realize that C isnt stably \n'...
'invertible: when computing the poles of C, we find that they are outside \n'...
'of the unit circle. This method can therefore not be used.\n'...
'\n'...
'Despite this, we can take advantage of knowing the correlation in the noise,\n'...
'and implement another method: the Best Linear Unbiased Estimator (BLUE or \n'...
'Markov estimate, slide 9.32). This method is ideal because it satisfies the \n'...
'requirements we are given: it computes an unbiased and minimum variance \n'...
'estimate theta_z of the parameters theta=[a1,a2,b1]:\n'...
'       > Unbiased estimate: this was proven in lecture with Prof. Smith. \n'...
'         We know that theta can be estimated by computing theta_z = Z^T * Y, \n'...
'         where:\n'...
'           > Z = R^(-1)*phi*( phi^T*R^(-1)*phi )^(-1)\n'...
'           > Y = phi^T*theta + epsilon\n'...
'           > R is the covariance matrix of errors\n'...
'         Mathematically, we can easily prove that the expectation of Z^T*Y \n'...
'         is equal to theta, proving therefore that the estimate is unbiased.\n'... 
'        > Minimum variance estimate: this was demonstrated in homework 7 \n'...
'         (problem 2.4). Indeed, the best linear unbiased estimate theta_z \n'...
'         has the smallest variance in the class of all unbiased estimators, \n'...
'         meaning cov(theta_z) <= cov(theta). This can be simply proven with \n'...
'         a minimization argument.\n'...
' \n'...
'BLUE is the best in the sense that it gives the minimum covariance in the \n'...
'estimate theta_z. In order to compute the BLUE, we have to find the covariance \n'...
'of errors R. If we had just a certain variance, R would be equal to this \n'...
'variance * identity. However, in our case, the noise has some correlation, \n'...
'meaning that the variance changes. \n'...
'After some mathematical derivations, the covariance matrix R is found to be \n'...
'the NxN matrix given by the following (where N is the length of p1_u):\n'...
'     lambda                if k=m=1\n'...
'     (1+c1^2)*lambda       if k=m=2\n'...
'     (1+c1^2+c2^2)*lambda  if k=m and k>=3\n'...
'     c1*lambda             if k=1, m=2 or if k=2, m=1\n'...
'     (c1+c1*c2)*lambda     if |k-m|=1 and min{k,m}>=2\n'...
'     c2*lambda             if |k-m|=2\n'...
'     0                     if |k-m|>=3\n'...
'Where the (k,m)-element of the matrix represents the element at the k-th \n'...
'row and m-th column, c1 and c2 are the coefficients of the transfer function \n'...
'C, and lambda is the variance of e(k). \n'...
'\n'...
'Furthermore, as we are looking to estimate the parameters theta=[a1,a2,b1],\n'...
'we can derive the regressor and find that it takes the following form: \n'...
'phi(k) = [-y(k-1)  -y(k-2)  u(k-1)]. As asked in the instructions, we start \n'...
'the regressor at k=2. Its 5 first rows are given by:\n'];
fprintf(explanation2);
phi_first_five_rows = phi(1:5,:)

explanation3 = [...
'\n'...
'Finally, we can compute the estimate theta_z of the parameters: \n'...
'theta_z = Z^T * Y, where Z was defined earlier. The following \n'...
'results are obtained:\n'...
'     > p1_a_est = [',num2str(p1_a_est(1)),' ',num2str(p1_a_est(2)),']\n'...
'     > p1_b_est = ',num2str(p1_b_est),'\n'];
fprintf(explanation3);
disp(' ');


%% --------------------------------------------------------------------- %%
%% PART 2 
%% --------------------------------------------------------------------- %%
disp(' ');
disp('#####################################################################')
disp('#                             PART 3:                               #')
disp('#                    Variance of of parameter b1                    #')
disp('#####################################################################')
disp(' ');

K = [7:1:N];
theta_est = zeros(length(K),nb_params);
beta_cov = zeros(length(K),1);
beta_cov2 = zeros(length(K),1);

%% Compute covariance matrix using the BLUE method
for n = K
    R_n = R(N-n+1:N-2,N-n+1:N-2);
    R_n_inv = R_n\eye(size(R_n));
    phi_n = phi(1:n-2,:);
    cov_inv = phi_n.'*R_n_inv*phi_n;
    theta_cov = cov_inv\eye(size(cov_inv));
    beta_cov(n-6) = theta_cov(3,3);
    
    phi2 = (phi_n.'*phi_n)\eye(size(phi_n.'*phi_n));
    theta_cov2 = phi2 * phi_n.' * R_n * phi_n * phi2;
    beta_cov2(n-6) = theta_cov2(3,3);
end

%% Return variance of b2 and plot
p1_b_var = beta_cov;

figure(1);
plot(K,p1_b_var,'b','linewidth',1); grid on; hold on;
plot(K,beta_cov2,'r--','linewidth',1);
legend('Variance of estimated b_1 computed with BLUE covariance formula',...
       'Variance of estimated b_1 computed with LS covariance formula',...
       'Location','northeast')
xlabel('K = 7,8,...,N');
ylabel('Variance of parameter b_1')
title('Variance of estimated b_1 as a function of K');

%% Documentation
explanation4 = [...
'In order to compute the variance of the estimated b1, we must first start \n'...
'by computing the covariance of the estimated parameters theta_z. To do so,\n'...
'the following formula is used:\n'...
'\n'...
'	 cov(theta_z) = ( phi^T * R^(-1) * phi )^(-1)\n'...
'\n'...
'Once the 3x3 covariance matrix is obtained, we find the variance of \n'...
'estimated b1 simply by extracting the (3,3) element of cov(theta_z).\n'...
'\n'...
'Lets also recall that since we are using BLUE, cov(theta_z) computed as \n'...
'shown above is smaller than cov(theta) for any unbiased estimate. This can \n'...
'be proved by computing the covariance using the formula for the least \n'...
'squares estimators:\n'...
'\n'...
'cov(theta) = ( phi^T*phi )^(-1) * phi^T * R * phi * ( phi^T*phi )^(-1)\n'...
'\n'...
'We also extract the variance of estimated b1 by taking the (3,3) element \n'...
'of cov(theta), and as expected, var(b1_z) is always smaller than var(b1).\n'...
'\n'...
'This can also be illustrated in the graph plotted in figure 1. This graph \n'...
'is obtained by computing the variance of estimated b1 as explained above, \n'...
'for different values of K. The red dotted line shows the variance of b1 \n'...
'computed with the LS covariance formula, while the blue full line shows \n'...
'the variance of b1 computed with the BLUE covariance formula. We clearly \n'...
'see here that whatever the value of K is, the variance obtained with BLUE \n'...
'is always smaller than the one obtained with LS, confirming what was stated \n'...
'earlier. \n'...
'Furthermore, we can also see that as K gets closer to N (length of p1_u), \n'...
'the variance decreases considerable. This is simply due to the fact that \n'...
'since we are using more and more data, the estimate becomes more and more \n'...
'refined and therefore the variance diminishes.\n'...
];
fprintf(explanation4);
disp(' ');



end